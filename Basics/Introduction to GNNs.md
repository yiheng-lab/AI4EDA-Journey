## 一、描述一个图
- **基本知识**
	- 描述一个图有V（顶点）和E（边），还有邻接矩阵，这些都是字面意思
	- GNN主要是GCN，图卷积神经网络，分空域和频域，空域就是上面直接用图的V和E去描述，涉及节点和边之间的信息交互，有点像LDPC的“置信度传播算法”，频域要对邻接矩阵做特征值分析
---
- **深入思考**
1. **如何表示PCB的“铺铜”？** 即，从一个驱动端同时连接到很多个负载端，且相互之间都是等电位的
	- Star Expansion，星型展开：源点Source像太阳，负载点Sink像行星。很省内存。但是，在这种结构下，Sink A 和 Sink B 之间没有直接连线。如果要传递信息，必须经过 Source，路径为A -> Source -> B。但是在真实的电路板或芯片上，这些Sink共享同一个电压电位。在物理布局时，我们希望这 100 个点不仅仅是离 Source 近，它们互相之间也要近。如果只用Star Expansion，GNN 可能会觉得 A 和 B 关系不大，导致布局的时候把 A 和 B 拉得很远，结果布线的时候线就不够用了。
		- GraphSAGE (SAmple and aggreGatE)
			- 有些点（比如时钟源）连了10万个点，计算它时，要聚合10万个邻居，显存直接爆炸
			- 解决方案是：随机采样。不管有多少相邻的点，每次只随机抽少量相邻的点来代表周围的点
			- 用来处理超大扇出节点
	- Clique Expansion，全连接：两两相连，费内存。
2. **对于有上亿个节点的图，邻接矩阵是存不下的**
	- 邻接表法：只存两个列表：
		- Row (起点): `[0, 0, 1, 2, ...]
		- Col (终点): `[1, 5, 2, 0, ...]`
		- 意思就是：节点 0 连向 1，0 连向 5，1 连向 2...
		- 这就是edge_index，边索引。 不管图有一亿个点还是十亿个点，只要边只有几百万条，我只存这几百万条记录，省内存
3. **空域VS频域**
	- 空域模拟的是“信息传递”，频域模拟的是“固有频率”
	- 频域 GNN 假设图的结构是固定的。但我们在 EDA 流程中，经常要根据预测结果去挪动节点、改变布线，图是动态变化的。所以AI4EDA的GNN一般用空域的。
	- GAT (Graph Attention Network)
		- GCN 认为所有邻居的重要性是一样的（取平均值）
		- 解决方案是：注意力机制。对于 EDA 来说，连在“关键路径（Critical Path）”上的邻居，肯定比普通邻居更重要，GAT 会给关键邻居更高的权重
		- 在时序分析里非常有用，因为时序只取决于那条最慢的路。

## 二、消息传递机制
- **基本知识**
	- 把每个节点所连接的节点的信息归一化后按边加权，然后重复迭代$$h_v^{(k)} = \sigma(W \cdot \sum_{u \in \mathcal{N}(v) \cup \{v\}} \frac{1}{\sqrt{d_v d_u}} h_u^{(l)})$$
	- 写成矩阵形式，为了并行计算：$$H^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)})$$含义：
		- 矩阵 $\tilde{A}$ 就是这个网表的**邻接矩阵**（加上了自环，即节点自己到自己的连线），如果逻辑门 $v$ 和逻辑门 $u$ 之间有导线相连，$\tilde{A}_{vu} = 1$，否则为 $0$。
		- 矩阵 $\tilde{D}$ 为**度矩阵**，对于第 $i$ 个节点，它的度 $d_i$ 就是与其相连的节点总数。度矩阵只在第 $i$ 行、第 $i$ 列有值，即 $d_i$ 
		- 矩阵 $H^{(l)}$ 里面每一行存的是一个逻辑门（节点）当前的特征向量 $h_v^{(l)}$。
		- 矩阵乘法得到的矩阵 $(\tilde{A}H^{(l)})$ 的第 $v$ 行是矩阵 $\tilde{A}$ 的第 $v$ 行，去点乘特征矩阵 $H^{(l)}$ 的每一列。又因为 $\tilde{A}$ 的第 $v$ 行里，只有节点 $v$ 的**邻居**（和它自己）对应的位置是 $1$，其余都是 $0$。所以，**去乘以 $H^{(l)}$ 的结果，恰好就是把 $v$ 的所有邻居（和自己）的特征向量原封不动地加起来。**
		- 用数学语言表达就是：$(\tilde{A}H^{(l)})_v = \sum_{u \in \mathcal{N}(v) \cup \{v\}} h_u^{(l)}$
		- 然后用$\tilde{D}$归一化：$(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)})_v = \sum_{u \in \mathcal{N}(v) \cup \{v\}} \frac{1}{\sqrt{d_v d_u}} h_u^{(l)}$
		- 最后乘以神经网络的权重矩阵 $W$
	- 为什么矩阵$A$和$D$要加上波浪线？
		- 波浪线代表着**加上“自环”（Self-loop）**
		- **$A$（原始邻接矩阵）：** 只记录了节点和邻居之间的连线。如果直接用 $A$ 去乘特征矩阵 $H$，节点在更新时，只吸收邻居的信息，却把自己上一刻的特征丢了。这不合理，节点自己的原始特征往往是最重要的。
		- **$\tilde{A}$（带自环的邻接矩阵）：** 为了让节点在聚合时也能带上自己，我们需要在图的每个节点上强制画一个圈，连向自己。在数学上，就是给原始的 $A$ 加上一个单位矩阵 $I$：$\tilde{A} = A + I$
		- **$\tilde{D}$（带自环的度矩阵）：** 既然每个节点都多了一条连向自己的边，那每个节点的“度数”自然也都增加了1。所以 $\tilde{D}$ 就是 $\tilde{A}$ 的度矩阵（主对角线元素等于 $\tilde{A}$ 每一行的和）
---
- **深入思考**
1. **感受野VS过度平滑**
	- GCN 的核心公式可以理解为不断左乘一个归一化的邻接矩阵，这里的 $\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$ 本质上是一个**低通滤波器**，当迭代次数 $l \to \infty$ 时，图中所有节点的特征向量会收敛到一个固定的子空间，节点信息趋同，相互之间不可区分，这就是过度平滑。
	- 所以：
		- **为了看清全局（大感受野）** $\to$ 需要增加层数
		- **增加层数** $\to$ 导致“加权求和”过度混合 $\to$ 所有节点特征趋同（过度平滑）
		- **结果** $\to$ 节点失去了区分度，分类或回归任务崩溃
	- 这就导致传统的 GCN 往往只能做到 2 到 4 层，一旦强行加深，性能不升反降
	- 解决办法有三种
		- [[Jumping Knowledge (JK-Net)]]：给节点加上“记忆锚点”
		- [[Graph Coarsening]]：图粗化，空间降维打击
		- [[Dilated GCN]]：空洞图卷积，跨级连接
2. **“权重”究竟如何理解？**
	- 在经典 GCN 里，边只负责两件事：1. 连通与否；2. 提供度数用来算归一化系数。这就好比我们看一张地铁线路图，只关心哪两站之间有线连着，至于这条线是高铁还是绿皮火车，GCN 完全不在乎，它眼里只有节点传递过来的特征 $h_u$。
	- 但是在电路网表转换成的图结构中，节点通常是标准单元（如逻辑门、触发器），而边是连接它们的导线，在深亚微米工艺下，导线早就不是理想的短路线了，它充满了复杂的寄生效应，比如，走在 M2 层（底层金属，又细又薄，电阻大）的线，和走在 M7 层（顶层厚金属，电阻小，适合跑全局时钟）的线，对信号延迟的衰减完全不同。
	- 如果用普通的 GCN，上游节点（发送方）把自己的特征 $h_u$（比如驱动能力、逻辑状态）传给下游节点时，经过一条 M2 的长线和经过一条 M7 的短线，得到的结果是一模一样的，因为 GCN 的公式里根本没有位置可以塞进这些物理参数。它完全丢失了物理电路的灵魂。
	- 解决方法：[[Edge-Conditioned MPNN]]：消息传递神经网络