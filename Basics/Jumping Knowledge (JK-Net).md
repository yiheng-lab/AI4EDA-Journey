- JK-Net 其实就是在GNN架构末端加了一个“旁路收集器”。对于图中的任意节点 $v$，它不只看最后一层的输出，而是将从输入层到第 $K$ 层的每一层特征 $h_v^{(1)}, h_v^{(2)}, \dots, h_v^{(K)}$ 都“跳跃（Jump）”连接到最后，末尾接一个聚合（Aggregate）函数
- 三种聚合函数
	- **Concatenation（拼接后线性变换 ）：** $$h_v^{(final)} = W \cdot \text{CONCAT}(h_v^{(1)}, h_v^{(2)}, \dots, h_v^{(K)})$$
		- **机制**：把所有层的特征首尾相接拼成一个长向量，然后通过一个可学习的权重矩阵 $W$ 进行降维。就像把所有层的报告装订在一起，交给最后的全连接层去通读，让神经网络通过训练，自己学习“哪一层的特征对当前节点最重要”。最简单粗暴，也最保留原始信息。
	- **Max-pooling（最大池化）：**$$h_v^{(final)} = \max(h_v^{(1)}, h_v^{(2)}, \dots, h_v^{(K)})$$
		- **机制**：在逐个特征维度上，取所有层中的最大值。
		- 极其强调“显著特征”。如果某个特征在浅层（局部）很突出，即使在深层被糊掉了，最大池化也能把它找回来。就像一个只看亮点的“一票否决/肯定制”。它不管这个特征是在第 1 层还是第 20 层提取的，只要某个特征维度在某一层表现得极其强烈，就直接把它提拔到最终的特征表示里。这在极其不规则的拓扑图中非常有效。
	- **LSTM-Attention（长短期记忆）：** 
		- **机制**：把节点在不同层的输出看作一个时间序列数据，输入到 LSTM 中，利用 LSTM 的门控机制（遗忘门、输入门等）来决定保留多少早期的“记忆锚点”。能够处理复杂的层间依赖关系，自适应能力最强。就像一个有记忆的序列分析专家。它把第 1 层到第 $K$ 层的特征看作一个时间序列，精细地判断信息是如何一步步演化的，哪一步的记忆该遗忘，哪一步该保留。虽然效果好，但计算开销最大。
- 电路网表（Netlist）本质上是一个高度异构、有向无环（DAG）且拓扑极不平衡的图。JK-Net 实现了节点级的**自适应感受野（Node-Adaptive Receptive Field）**。
	- **局部高频特征**（浅层网络负责）：比如，当前节点到底是个与非门还是反相器？它的晶体管尺寸多大？它的本征延迟和引脚电容是多少？这是计算电路延迟的物理基础。如果丢了这些信息，时序分析无从谈起。JK-Net通过保留 Layer 0/1 的特征，无论 GNN 叠加到多少层，节点都不会“忘了自己是谁”。
	- **全局低频特征**（深层网络负责）：当前节点的扇入锥有多庞大？节点是否处于一条长达几十级的关键路径上？这是决定一个门电路时序裕量的宏观拓扑结构。JK-Net允许网络堆叠到 20 层甚至 30 层以覆盖长路径，捕捉到这部分宏观拓扑信息。
	- 在同一个电路中，有的路径只有 3 级逻辑门，有的路径有 50 级。 如果使用固定的深层 GNN，短路径上的门会严重过度平滑；如果使用浅层 GNN，长路径上的门则视野受限。**JK-Net 允许短路径上的节点赋予浅层特征更高权重，长路径上的节点赋予深层特征更高权重，契合了电路图的极度不规则性。**