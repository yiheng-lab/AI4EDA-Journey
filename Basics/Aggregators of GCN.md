- **三个典型例子**
	- 拥塞预测用`Max`，因为在布线问题中，局部的极端恶化决定了全局的失败。
	- 面积与功耗用`Sum`，因为在综合或布局阶段，我们需要评估当前网表的总面积和总功耗。面积和功耗是加性物理量，聚合邻居信息时，必须把所有的消耗累加起来。
	- 时序分析
		- Setup Time（建立时间）约束：信号到达终点必须早于某个时钟沿。这时候，决定信号能不能按时到的，是最慢的那条路。所以，向后传递消息时必须使用 **`Max`** 聚合器提取最大延迟。
		- Hold Time（保持时间）约束：信号不能跑太快，否则会冲刷掉上一周期的数据。决定信号会不会跑太快的，是最快的那条路。这时候，消息传递必须使用 **`Min`** 聚合器提取最小延迟。
- **GAT（Graph Attention Network）：让模型自己学习权重**$$h_v^{(k)} = \sigma \left( \sum_{u \in \mathcal{N}(v)} \alpha_{u,v} W h_u^{(k-1)} \right)$$
	- 在先进工艺（如 3nm/5nm）下，相邻的线网之间会产生耦合电容，旁边的线如果发生电平跳变，会把主信号线的信号拖慢。
	- 也就是说
		- 用 `Max`：丢掉了旁边邻居的串扰信息。
		- 用 `Sum`：把主次关系全搞乱了。
	- 如果改用GAT：对于那个最致命的 Critical Path 邻居，模型可能会学到一个极大的权重，对于旁边那些只会产生微弱串扰的邻居，模型会赋予它们微小的权重，GAT契合了先进工艺下复杂的物理耦合效应。
- **PNA (Principal Neighbourhood Aggregation)：多聚合器拼接**
	- PNA很强大也很抽象，一步步来
	- 首先，GNN 所谓的“聚合 (Aggregation)”，数学上的本质，就是**把一个多重集压缩成一个固定长度的向量**，同时尽可能不丢失这个多重集里的信息
		- 为什么它叫“多重集 (Multiset)”，而不是普通的“集合 (Set)”？
			- 普通集合的规则是：不能有重复元素。 如果节点A的两个邻居B和C的特征完全一样（比如都是表示某个相同类型的逻辑门向量 `[1, 0, 0]`），在普通集合里，`{[1, 0, 0], [1, 0, 0], 向量D}` 会被强制去重，变成 `{[1, 0, 0], 向量D}`。
			- 但是，如果去重了，节点 A 就会以为自己只有两个邻居，它彻底丢失了图的拓扑结构（度数信息）。
			- 因此，节点的邻居信息，必须是一个允许元素重复的集合，即多重集
		- 怎么保证压缩后，两个不同的多重集不会变成同一个向量？
			- 可以把“多重集”看成空间中散落的一堆数据点（一个离散的数据分布），只要掌握了一个分布的所有矩，就能唯一确定这个分布
				- 0 阶矩（对应 Sum/Count）：描述这堆数据有多少个。
				- 1 阶矩（对应 Mean）：描述这堆数据的中心位置在哪里。
				- 2 阶矩（对应 Std，标准差）：描述这堆数据是紧凑还是分散。
				- 更高阶矩（如偏度、峰度等）：描述数据分布的形状偏向。
				- 无穷阶矩（对应 Max/Min）：描述这堆数据的绝对边界。
			- **GCN（只用 Mean）：** 它只提取了“1阶矩”。如果两个节点的邻居分布，一个是全在均值附近，另一个是极端两极分化（但均值相同），GCN 会认为这两个邻居群是一模一样的。
			- **[GIN](GIN.md)（只用 Sum）：** 它提取了“0阶矩”和部分位置信息。虽然数学上证明了它很强，但在实际计算中，极其容易因为邻居太多导致数值爆炸，或者被掩盖掉局部的方差特征。
		- PNA 的策略就是：既然一个统计量（一个矩）无法完整描述这个多重集，那我就全要了：Mean（1阶矩，看中心），Std（2阶矩，看离散程度），Max / Min（无穷阶矩，看边界极值）
	- PNA还设计了一个“对数度缩放器”
		- 一个现实问题：为什么 GNN 在处理真实世界的图时，经常会“学傻”？
		- 因为真实世界的图（无论是社交网络、分子结构还是电路网表）几乎都服从幂律分布。绝大多数节点只有几个邻居，但极少数节点有成千上万个邻居。
			- 如果用 Sum 聚合，大节点的数值会爆炸，把神经网络的激活函数击穿
			- 如果用 Mean 聚合，大节点的特征会被除以一万，变成白噪音，模型根本察觉不到它的重要性
		- PNA的方法是，引入一个缩放器：既然节点度数 $d$ 差异这么大，那我就在聚合完特征后，再乘上一个系数，把特征按比例“放大”或“缩小”。但如果直接用度数 $d$ 去乘，数值还是会爆炸。所以PNA用了一个极其优美的数学工具：对数 。对数天生就是用来把天文数字压缩到人类可读范围的。
		- PNA定义了一个缩放因子 $S$：$$S(d, \alpha) = \left( \frac{\log(d+1)}{\log(\delta+1)} \right)^\alpha$$$d$ 是当前节点的度数，$\delta$ 是整个图的平均度数（作为基准）。$\alpha$ 是控制开关：
			- $\alpha = 0$ (恒等)： $S = 1$。什么都不做，保留原始的聚合结果。
			- $\alpha = 1$ (放大)：当你的度数 $d$ 大于平均值 $\delta$ 时，$S > 1$。物理意义是：告诉模型这个节点是个大枢纽，不要因为做了 Mean 聚合就把它的信号当成普通噪音忽略掉。
			- $\alpha = -1$ (衰减)：当你的度数 $d$ 大于平均值 $\delta$ 时，$S < 1$。物理意义是：告诉模型这个节点连接的东西太多了，它的 Sum 聚合结果可能已经虚高，要压制一下它的权重。
		- 通过对数平滑，模型完美地驯服了那些动辄连接上万条边的“超级节点”。
	- PNA 的单层更新公式并不是简单地相加，而是将聚合器与缩放器进行张量积$$\text{Aggregation} = \begin{bmatrix} 1 \\ S(d, 1) \\ S(d, -1) \end{bmatrix} \otimes \begin{bmatrix} \mu \\ \sigma \\ \max \\ \min \end{bmatrix}$$这就意味着，对于每个节点的局部邻域，PNA 会生成 $3 \times 4 = 12$ 个不同的统计向量，然后将它们拼接起来，最后通过一个多层感知机进行特征降维和非线性变换：$$h_i^{(l+1)} = \text{MLP} \left( h_i^{(l)}, \bigoplus_{j \in \mathcal{N}(i)} \text{Message}(h_i^{(l)}, h_j^{(l)}) \right)$$
